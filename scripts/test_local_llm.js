/**
 * Script de Prueba para IA Local (Ollama)
 * 
 * Este script valida:
 * 1. Conectividad con el servicio Ollama
 * 2. Disponibilidad del modelo configurado
 * 3. ClasificaciÃ³n de intenciÃ³n con modelo local
 * 4. Fallback automÃ¡tico a OpenAI
 * 5. ComparaciÃ³n de latencia y precisiÃ³n
 */

require('dotenv').config();
const fs = require('fs');
const path = require('path');

// Cargar configuraciÃ³n
const configPath = path.join(__dirname, '../bot_base/config.json');
const config = JSON.parse(fs.readFileSync(configPath, 'utf-8'));

// Cargar mÃ³dulos
const { checkLocalServiceHealth, classifyIntentLocal, getLocalServiceStatus } = require('../engine/local_llm');
const { classifyIntent } = require('../engine/llm');

// Casos de prueba
const testCases = [
  // Casos SIMPLE
  { question: "Hola, Â¿cÃ³mo estÃ¡s?", expected: "SIMPLE", description: "Saludo bÃ¡sico" },
  { question: "Gracias por tu ayuda", expected: "SIMPLE", description: "Agradecimiento" },
  { question: "Â¿QuÃ© puedes hacer?", expected: "SIMPLE", description: "Pregunta sobre el bot" },
  { question: "AdiÃ³s", expected: "SIMPLE", description: "Despedida" },
  
  // Casos COMPLEX
  { question: "Â¿QuÃ© dice el artÃ­culo 1545 del CÃ³digo Civil?", expected: "COMPLEX", description: "Consulta de artÃ­culo especÃ­fico" },
  { question: "Â¿CuÃ¡les son los requisitos para un contrato vÃ¡lido?", expected: "COMPLEX", description: "Pregunta jurÃ­dica general" },
  { question: "ExplÃ­came la prescripciÃ³n adquisitiva", expected: "COMPLEX", description: "Concepto jurÃ­dico" },
  { question: "Â¿CÃ³mo se calcula la indemnizaciÃ³n por despido?", expected: "COMPLEX", description: "CÃ¡lculo legal" },
];

async function runTests() {
  console.log('â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•');
  console.log('  TEST DE IA LOCAL (OLLAMA) - LexCode V8.3');
  console.log('â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n');
  
  // Verificar configuraciÃ³n
  console.log('ğŸ“‹ ConfiguraciÃ³n de IA Local:');
  console.log(`   - IA Local habilitada: ${config.enable_local_llm}`);
  console.log(`   - Host Ollama: ${config.local_llm_host || 'http://localhost:11434'}`);
  console.log(`   - Modelo: ${config.local_llm_model || 'llama3.2:3b'}`);
  console.log(`   - Fallback a OpenAI: AutomÃ¡tico`);
  console.log('');
  
  // PASO 1: Verificar conectividad con Ollama
  console.log('ğŸ” PASO 1: Verificando conectividad con Ollama...\n');
  
  const isHealthy = await checkLocalServiceHealth(config);
  
  if (!isHealthy) {
    console.log('âŒ ADVERTENCIA: Servicio Ollama no disponible');
    console.log('   El sistema usarÃ¡ fallback automÃ¡tico a OpenAI');
    console.log('');
    console.log('ğŸ“ Para instalar Ollama:');
    console.log('   1. Visita: https://ollama.ai');
    console.log('   2. Descarga e instala Ollama para tu sistema operativo');
    console.log('   3. Ejecuta: ollama pull llama3.2:3b');
    console.log('   4. Verifica: ollama list');
    console.log('');
    console.log('âš ï¸  Continuando con pruebas usando fallback a OpenAI...\n');
  } else {
    console.log('âœ… Servicio Ollama disponible y funcionando\n');
  }
  
  // PASO 2: Probar clasificaciÃ³n con modelo local (o fallback)
  console.log('ğŸ§ª PASO 2: Probando clasificaciÃ³n de intenciÃ³n...\n');
  
  let passed = 0;
  let failed = 0;
  let localUsed = 0;
  let fallbackUsed = 0;
  const results = [];
  
  for (let i = 0; i < testCases.length; i++) {
    const test = testCases[i];
    console.log(`[Test ${i + 1}/${testCases.length}] ${test.description}`);
    console.log(`Pregunta: "${test.question}"`);
    console.log(`Esperado: ${test.expected}`);
    
    try {
      const startTime = Date.now();
      const classification = await classifyIntent(config, test.question);
      const duration = Date.now() - startTime;
      
      const success = classification === test.expected;
      
      // Verificar si se usÃ³ modelo local o fallback
      const status = getLocalServiceStatus();
      const usedLocal = isHealthy && config.enable_local_llm === true;
      
      if (usedLocal) {
        localUsed++;
      } else {
        fallbackUsed++;
      }
      
      if (success) {
        console.log(`âœ… PASS - ClasificaciÃ³n: ${classification} (${duration}ms) ${usedLocal ? '[LOCAL]' : '[OPENAI]'}`);
        passed++;
      } else {
        console.log(`âŒ FAIL - ClasificaciÃ³n: ${classification} (esperado: ${test.expected})`);
        failed++;
      }
      
      results.push({
        test: test.description,
        question: test.question,
        expected: test.expected,
        actual: classification,
        success: success,
        duration: duration,
        usedLocal: usedLocal
      });
      
      console.log('');
      
    } catch (error) {
      console.log(`âŒ ERROR - ${error.message}\n`);
      failed++;
      results.push({
        test: test.description,
        question: test.question,
        expected: test.expected,
        actual: 'ERROR',
        success: false,
        error: error.message
      });
    }
  }
  
  // PASO 3: Resumen
  console.log('â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•');
  console.log('ğŸ“Š RESUMEN DE PRUEBAS:');
  console.log(`   âœ… Exitosas: ${passed}/${testCases.length} (${((passed/testCases.length)*100).toFixed(1)}%)`);
  console.log(`   âŒ Fallidas: ${failed}/${testCases.length}`);
  console.log('');
  console.log('ğŸ“ˆ Uso de Modelos:');
  console.log(`   ğŸ  Modelo Local: ${localUsed}/${testCases.length} clasificaciones`);
  console.log(`   â˜ï¸  OpenAI (Fallback): ${fallbackUsed}/${testCases.length} clasificaciones`);
  
  // AnÃ¡lisis de latencia
  const durations = results.filter(r => r.duration).map(r => r.duration);
  if (durations.length > 0) {
    const avgDuration = durations.reduce((a, b) => a + b, 0) / durations.length;
    const minDuration = Math.min(...durations);
    const maxDuration = Math.max(...durations);
    
    console.log('');
    console.log('â±ï¸  AnÃ¡lisis de Latencia:');
    console.log(`   - Promedio: ${avgDuration.toFixed(0)}ms`);
    console.log(`   - MÃ­nimo: ${minDuration}ms`);
    console.log(`   - MÃ¡ximo: ${maxDuration}ms`);
  }
  
  // AnÃ¡lisis de costos
  console.log('');
  console.log('ğŸ’° AnÃ¡lisis de Costos:');
  console.log(`   - Clasificaciones con modelo local: ${localUsed} Ã— $0.00 = $0.00`);
  console.log(`   - Clasificaciones con OpenAI: ${fallbackUsed} Ã— ~$0.0001 = ~$${(fallbackUsed * 0.0001).toFixed(4)}`);
  console.log(`   - Ahorro total: ~$${(localUsed * 0.0001).toFixed(4)}`);
  
  // Estado del servicio local
  console.log('');
  console.log('ğŸ”§ Estado del Servicio Local:');
  const serviceStatus = getLocalServiceStatus();
  console.log(`   - Disponible: ${serviceStatus.available !== false ? 'SÃ­' : 'No'}`);
  console.log(`   - Ãšltima verificaciÃ³n: ${serviceStatus.lastCheck > 0 ? new Date(serviceStatus.lastCheck).toLocaleString() : 'Nunca'}`);
  
  // Guardar resultados
  const resultsPath = path.join(__dirname, '../temp/test_local_llm_results.json');
  fs.writeFileSync(resultsPath, JSON.stringify({
    timestamp: new Date().toISOString(),
    config: {
      local_llm_enabled: config.enable_local_llm,
      local_llm_host: config.local_llm_host,
      local_llm_model: config.local_llm_model,
      ollama_available: isHealthy
    },
    summary: {
      total: testCases.length,
      passed: passed,
      failed: failed,
      success_rate: ((passed/testCases.length)*100).toFixed(1) + '%',
      local_used: localUsed,
      fallback_used: fallbackUsed
    },
    results: results
  }, null, 2));
  
  console.log(`\nğŸ’¾ Resultados guardados en: ${resultsPath}`);
  console.log('â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n');
  
  // Recomendaciones finales
  if (!isHealthy) {
    console.log('ğŸ’¡ RECOMENDACIÃ“N:');
    console.log('   Para aprovechar el ahorro de costos con IA local:');
    console.log('   1. Instala Ollama desde https://ollama.ai');
    console.log('   2. Descarga el modelo: ollama pull llama3.2:3b');
    console.log('   3. Reinicia el servidor de LexCode');
    console.log('   4. Ejecuta este script nuevamente para verificar');
    console.log('');
  } else {
    console.log('ğŸ‰ Â¡Ã‰XITO!');
    console.log('   El sistema estÃ¡ usando IA local para clasificaciÃ³n');
    console.log('   Costo de clasificaciÃ³n: $0 (100% de ahorro)');
    console.log('');
  }
  
  // Exit code
  process.exit(failed > 0 ? 1 : 0);
}

// Ejecutar pruebas
runTests().catch(error => {
  console.error('âŒ Error fatal en las pruebas:', error);
  process.exit(1);
});

